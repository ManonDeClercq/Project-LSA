# Import library
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Save filepath to variable for easier access
metadata_file_path = r'/kaggle/input/metadata/metadata.csv'
abundances_file_path = r'/kaggle/input/abundances/abundances.xlsx'

# Read the data and store data in a DataFrame
metadata = pd.read_csv(metadata_file_path)
abundances = pd.read_excel(abundances_file_path)

# Print a summary of the data
print(metadata.describe())
print(abundances.describe())


#combine datasets

# Combine datasets based on index
combined_data = pd.concat([metadata, abundances.reset_index()], axis=1)

# Inspect combined dataset
print(combined_data.head())

#0 for missing values
combined_data.fillna(0, inplace=True)

# Define target variable and features
target = 'Diameter_of_tumor_(cm)'
y = combined_data[target]  # Target variable
X = combined_data[['AFP_(ng/ml)', 'Macrovascular_invasion']]  # Features


#split data in training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Make data like '1.2+1.5' as mean:
def parse_to_float(value):
    try:
        # Check if '+' is in the string and calculate the mean of the numbers
        if '+' in str(value):
            numbers = [float(v) for v in value.split('+')]
            return sum(numbers) / len(numbers)
        else:
            return float(value)  # Convert directly to float if no '+'
    except ValueError:
        return np.nan  # Return NaN for invalid values

# Apply parsing 
y_train = y_train.apply(parse_to_float)
y_test = y_test.apply(parse_to_float)
y = y.apply(parse_to_float)
X = X.apply(parse_to_float)


#0 for missing values
X_train.fillna(0, inplace=True)
X_test.fillna(0, inplace=True)

#DescisionTreeRegressor model #model1
from sklearn.tree import DecisionTreeRegressor

# Define model. Specify a number for random_state to ensure same results each run
melbourne_model = DecisionTreeRegressor(random_state=1)

# Fit model
melbourne_model.fit(X, y)


print(X.head())
print("The predictions are")
print(melbourne_model.predict(X.head()))


#RandomForestRegressor model #model2
# Train een Random Forest Regressor model
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Predictions 
y_pred = model.predict(X_test)

# Evaluate model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R² Score: {r2:.2f}")

# Visualise important features
import matplotlib.pyplot as plt

feature_importances = pd.Series(model.feature_importances_, index=X.columns)
feature_importances.nlargest(2).plot(kind="barh")
plt.title("Feature Importances")
plt.show()









#updated code
#Only on tumor data from abundances
# Import libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
import numpy as np
import matplotlib.pyplot as plt

# File paths
metadata_file_path = r'/kaggle/input/metadata/metadata.csv'
abundances_file_path = r'/kaggle/input/abundances/abundances.xlsx'

# Read data
metadata = pd.read_csv(metadata_file_path)
abundances = pd.read_excel(abundances_file_path)

# Combine datasets
combined_data = pd.concat([metadata, abundances.reset_index()], axis=1)

# Fill missing values with 0
combined_data.fillna(0, inplace=True)

#Remove LxxxP data
patient_columns = [col for col in abundances.columns if col.endswith('P') and col.startswith('L')]
abundances_dropped = abundances.drop(columns=patient_columns)

# Define target variable and features
target = 'Diameter_of_tumor_(cm)'
y = combined_data[target]  # Target variable
X = combined_data[['AFP_(ng/ml)', 'MVI', 'age', 'Tumor_number']]  # Features

# Apply Label Encoding to 'Protein.names' column
label_encoder = LabelEncoder()
X['Protein.names'] = label_encoder.fit_transform(combined_data['Protein.names'])

# Rename columns for better visualization in the feature importance graph
X.rename(columns={
    'AFP_(ng/ml)': 'AFP',
    'MVI': 'MVI',
    'age': 'Age',
    'Tumor_number': 'Tumor Number'
}, inplace=True)

# Function to handle '+' in string and convert to float
def parse_to_float(value):
    try:
        # Check if '+' is in the string and calculate the mean of the numbers
        if '+' in str(value):
            numbers = [float(v) for v in value.split('+')]
            return sum(numbers) / len(numbers)
        else:
            return float(value)  # Convert directly to float if no '+'
    except ValueError:
        return np.nan  # Return NaN for invalid values

# Apply parse_to_float function to the entire dataframe
X = X.applymap(parse_to_float)
y = y.apply(parse_to_float)

# Drop rows with NaN values
combined_data = pd.concat([X, y], axis=1)
combined_data.dropna(inplace=True)

# Re-separate X and y after dropping rows with NaN values
X = combined_data.drop(columns=[target])
y = combined_data[target]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train DecisionTreeRegressor model (model 1)
melbourne_model = DecisionTreeRegressor(random_state=1)
melbourne_model.fit(X_train, y_train)

# Make predictions with DecisionTreeRegressor
y_pred_tree = melbourne_model.predict(X_test)

# Print predictions and evaluate the model
print("Decision Tree Predictions:", y_pred_tree)

# Initialize and train RandomForestRegressor model (model 2)
random_forest_model = RandomForestRegressor(random_state=42)
random_forest_model.fit(X_train, y_train)

# Make predictions with RandomForestRegressor
y_pred_forest = random_forest_model.predict(X_test)

# Evaluate RandomForestRegressor model
mse = mean_squared_error(y_test, y_pred_forest)
r2 = r2_score(y_test, y_pred_forest)
print(f"Random Forest - Mean Squared Error: {mse:.2f}")
print(f"Random Forest - R² Score: {r2:.2f}")

# Visualize feature importances from RandomForestRegressor
feature_importances = pd.Series(random_forest_model.feature_importances_, index=X.columns)
feature_importances.plot(kind="barh", color='skyblue')
plt.title("Feature Importances")
plt.xlabel("Importance")
plt.ylabel("Features")
plt.show()
